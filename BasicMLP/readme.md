- The directory structure of BasicMLP is as follows:

    ```plain text
    BasicMLP
        └─img_md [Images used in markdown documents]
            └─main-Numpy.py
            └─main-Pytorch.py
            └─readme.md
    ```

    





# 一、使用NumPy手写简单的多层感知机——MLP

## ① 提示

- 这里只是随便举了一个例子，来体验神经网络的整个过程。

## ② 训练任务
- 输入为两个数字，标签为两个数字的和，然后进行训练，让 MLP 从中学习特征，来计算两个数字的加法。
- 测试输入两个数字的预测结果是否接近于两个数字的和。

## ③ 模型结构——双层感知机

<img src="img_md\双层感知机.png" style="zoom:33%;" />

## ④ 模拟前向传播
- 全连接的隐层计算：包含两个全连接：输入层 —— 隐藏层 & 隐藏层 —— 输出层
    $$
    y = Wx + b
    $$
    
- 激活函数的计算：隐藏层后使用激活函数，由于本任务的特殊性所有输出层没有使用激活函数。
    $$
    Sigmod(x) = \sigma(x)= \frac{1}{1 + e^-x}
    $$
    
- 连续变量的损失值的计算（均方误差）：
    $$
    MSE = \frac{1}{n} \sum_{i=1}^{n}(\widehat{y}_i-y_i)^2
    $$

## ⑤ 模拟反向传播
- 计算梯度(是沿着梯度的反方向前进)

    - 这里使用的不是标准的反向传播。使用**数值差分（finite difference）**的方法来估计每个参数的梯度，然后做一次梯度下降更新。
        $$
        \frac{\delta L}{\delta w} \approx \frac{L(w + h) - L(w)}{h}
        $$

        - h 表示数值差分的步长，用于近似导数。

- 更新权重参数

## ⑥ 使用训练好的权重参数进行预测

# 二、使用PyTorch写简单的多层感知机——MLP

## ① 提示

- 这里只是随便举了一个例子，来体验神经网络的整个过程。

## ② 训练任务

- 输入为两个数字，标签为两个数字的和，然后进行训练，让 MLP 从中学习特征，来计算两个数字的加法。
- 测试输入两个数字的预测结果是否接近于两个数字的和。

## ③ 定义自己的Dataset（如何批处理数据） 

## ④ 模型结构——双层感知机

<img src="img\双层感知机.png" style="zoom:33%;" />

## ⑤ 前向传播（计算预测值）

## ⑥ 反向传播

- 选择损失函数和优化器
- 计算损失值直接调用函数
- 计算梯度直接调用函数
- 更新权重直接调用函数

## ⑦ 使用训练好的权重参数进行预测
